{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed89fdef",
   "metadata": {},
   "source": [
    "# Parse Cosmopedia for contexts of simple words\n",
    "Here, we define simple words as the first words that children learn to produce according to the MacArther-Bates CDI. These words can be found in `data/wordbank_item_data.csv` or https://wordbank.stanford.edu/data/.\n",
    "\n",
    "We chose Cosmopedia as our text dataset due to its larger variety of topics. Additionally, it is a synthetic dataset, which ensures there is no data leakage in the training text (OpenWebText)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f30e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "import csv\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "X = 128\n",
    "TOKEN_LIMIT = 500_000_000\n",
    "WORDBANK_PATH = \"data/wordbank_item_data.csv\"\n",
    "OUTPUT_PATH = \"data/contexts_cosmopedia.pkl\"\n",
    "\n",
    "COSMOPEDIA_SUBSETS = [\n",
    "    \"auto_math_text\",\n",
    "    \"khanacademy\",\n",
    "    \"openstax\",\n",
    "    \"stanford\",\n",
    "    \"stories\",\n",
    "    \"web_samples_v1\",\n",
    "    \"web_samples_v2\",\n",
    "    \"wikihow\",\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ab504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text: str) -> List[str]:\n",
    "    return re.findall(r\"[A-Za-z]+\", text.lower())\n",
    "\n",
    "\n",
    "def sample_cosmopedia_texts(token_limit: int = TOKEN_LIMIT, subsets: List[str] = None) -> List[str]:\n",
    "    if subsets is None:\n",
    "        subsets = COSMOPEDIA_SUBSETS\n",
    "\n",
    "    per_subset_limit = max(token_limit // len(subsets), 1)\n",
    "    texts: List[str] = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for subset in subsets:\n",
    "        subset_tokens = 0\n",
    "        ds = load_dataset(\"HuggingFaceTB/cosmopedia\", subset, split=\"train\", streaming=True)\n",
    "        for item in ds:\n",
    "            text = item[\"text\"]\n",
    "            words = count_words(text)\n",
    "            n = len(words)\n",
    "            if n == 0:\n",
    "                continue\n",
    "            texts.append(text)\n",
    "            subset_tokens += n\n",
    "            total_tokens += n\n",
    "            if subset_tokens >= per_subset_limit or total_tokens >= token_limit:\n",
    "                break\n",
    "\n",
    "        if total_tokens >= token_limit:\n",
    "            break\n",
    "\n",
    "    print(f\"Collected {len(texts)} documents, approx {total_tokens} word tokens.\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_wordbank_words(path: str = \"data/wordbank_item_data.csv\") -> set:\n",
    "    allowed = set()\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            term = row[\"item_definition\"].strip().lower()\n",
    "            term = re.sub(r\"\\s*\\([^)]*\\)\", \"\", term).strip()\n",
    "            if term:\n",
    "                allowed.add(term)\n",
    "    return allowed\n",
    "\n",
    "\n",
    "def get_corpus_vocab(texts: List[str]) -> set:\n",
    "    vocab = set()\n",
    "    for text in texts:\n",
    "        vocab.update(count_words(text))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def find_simple_words(texts: List[str], wordbank_path: str = \"data/wordbank_item_data.csv\") -> set:\n",
    "    allowed_words = load_wordbank_words(wordbank_path)\n",
    "    corpus_vocab = get_corpus_vocab(texts)\n",
    "    return corpus_vocab & allowed_words\n",
    "\n",
    "\n",
    "def collect_contexts_from_texts(\n",
    "    texts: List[str],\n",
    "    simple_words: set,\n",
    "    max_context: int = X,\n",
    "    window_size: int = 10,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \n",
    "    contexts = defaultdict(set)\n",
    "\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=32), total=len(texts), desc=\"Collecting contexts\"):\n",
    "        for sent in doc.sents:\n",
    "            words = [tok.text.lower() for tok in sent if tok.is_alpha]\n",
    "            for i, w in enumerate(words):\n",
    "                if w in simple_words and len(contexts[w]) < max_context:\n",
    "                    start = max(0, i - window_size)\n",
    "                    prefix = \" \".join(words[start:i])\n",
    "\n",
    "                    if prefix:\n",
    "                        contexts[w].add(prefix)\n",
    "\n",
    "    return {w: list(contexts[w]) for w in simple_words if len(contexts[w]) >= max_context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49af1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sample_cosmopedia_texts(token_limit=TOKEN_LIMIT, subsets=COSMOPEDIA_SUBSETS)\n",
    "simple_words = find_simple_words(texts, wordbank_path=WORDBANK_PATH)\n",
    "print(f\"Found {len(simple_words)} simple words.\")\n",
    "contexts = collect_contexts_from_texts(texts, simple_words, max_context=X)\n",
    "print(f\"{len(contexts)} words have at least {X} contexts.\")\n",
    "with open(OUTPUT_PATH, \"wb\") as f:\n",
    "    pickle.dump(contexts, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
