{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769a2639",
   "metadata": {},
   "source": [
    "# GPT-2 vs. Child Word Acquisition\n",
    "To skip to Plots section, download files here: https://drive.google.com/drive/folders/1gHxewyharnFG-9vuE4CX2FQxhPGzTvM1?usp=sharing\n",
    "- `wordbank_item_data.csv`\n",
    "- `contexts_cosmopedia.pkl`\n",
    "- `stanford-gpt2-small-a_results.zip`\n",
    "- `stanford-gpt2-medium-a_results.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89fdef",
   "metadata": {},
   "source": [
    "## Parse Cosmopedia for contexts of simple words\n",
    "Here, we define simple words as the first words that children learn to produce according to the MacArther-Bates CDI. These words can be found in `data/wordbank_item_data.csv` or https://wordbank.stanford.edu/data/.\n",
    "\n",
    "We chose Cosmopedia as our text dataset due to its larger variety of topics. Additionally, it is a synthetic dataset, which ensures there is no data leakage in the training text (OpenWebText).\n",
    "\n",
    "We parse all Cosmopedia subsets for simple words and their contexts (text prefix of `window_size=10`).\n",
    "\n",
    "For a faster runtime, decrease `TOKEN_LIMIT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f30e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "import csv\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "X = 128\n",
    "TOKEN_LIMIT = 200_000\n",
    "WORDBANK_PATH = \"data/wordbank_item_data.csv\"\n",
    "CONTEXTS_PKL_PATH = \"data/contexts_cosmopedia.pkl\"\n",
    "\n",
    "COSMOPEDIA_SUBSETS = [\n",
    "    \"auto_math_text\",\n",
    "    \"khanacademy\",\n",
    "    \"openstax\",\n",
    "    \"stanford\",\n",
    "    \"stories\",\n",
    "    \"web_samples_v1\",\n",
    "    \"web_samples_v2\",\n",
    "    \"wikihow\",\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ab504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text: str):\n",
    "    return re.findall(r\"[A-Za-z]+\", text.lower())\n",
    "\n",
    "\n",
    "def sample_cosmopedia_texts(token_limit: int = TOKEN_LIMIT, subsets: List[str] = None):\n",
    "    if subsets is None:\n",
    "        subsets = COSMOPEDIA_SUBSETS\n",
    "\n",
    "    per_subset_limit = max(token_limit // len(subsets), 1)\n",
    "    texts: List[str] = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for subset in subsets:\n",
    "        subset_tokens = 0\n",
    "        ds = load_dataset(\"HuggingFaceTB/cosmopedia\", subset, split=\"train\", streaming=True)\n",
    "        for item in ds:\n",
    "            text = item[\"text\"]\n",
    "            words = count_words(text)\n",
    "            n = len(words)\n",
    "            if n == 0:\n",
    "                continue\n",
    "            texts.append(text)\n",
    "            subset_tokens += n\n",
    "            total_tokens += n\n",
    "            if subset_tokens >= per_subset_limit or total_tokens >= token_limit:\n",
    "                break\n",
    "\n",
    "        if total_tokens >= token_limit:\n",
    "            break\n",
    "\n",
    "    print(f\"Collected {len(texts)} documents, approx {total_tokens} word tokens.\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_wordbank_words(path: str = \"data/wordbank_item_data.csv\"):\n",
    "    allowed = set()\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            term = row[\"item_definition\"].strip().lower()\n",
    "            term = re.sub(r\"\\s*\\([^)]*\\)\", \"\", term).strip()\n",
    "            if term:\n",
    "                allowed.add(term)\n",
    "    return allowed\n",
    "\n",
    "\n",
    "def get_corpus_vocab(texts: List[str]):\n",
    "    vocab = set()\n",
    "    for text in texts:\n",
    "        vocab.update(count_words(text))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def find_simple_words(texts: List[str], wordbank_path: str = \"data/wordbank_item_data.csv\"):\n",
    "    allowed_words = load_wordbank_words(wordbank_path)\n",
    "    corpus_vocab = get_corpus_vocab(texts)\n",
    "    return corpus_vocab & allowed_words\n",
    "\n",
    "\n",
    "def collect_contexts_from_texts(\n",
    "    texts: List[str],\n",
    "    simple_words: set,\n",
    "    max_context: int = X,\n",
    "    window_size: int = 10,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \n",
    "    contexts = defaultdict(set)\n",
    "    completed_words = set()\n",
    "\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=32), total=len(texts), desc=\"Collecting contexts\"):\n",
    "        for sent in doc.sents:\n",
    "            words = [tok.text.lower() for tok in sent if tok.is_alpha]\n",
    "            for i, w in enumerate(words):\n",
    "                if w in simple_words and len(contexts[w]) < max_context:\n",
    "                    start = max(0, i - window_size)\n",
    "                    prefix = \" \".join(words[start:i])\n",
    "\n",
    "                    if prefix:\n",
    "                        contexts[w].add(prefix)\n",
    "\n",
    "                    if len(contexts[w]) >= max_context:\n",
    "                        completed_words.add(w)\n",
    "                        \n",
    "                if len(completed_words) == len(simple_words):\n",
    "                    break\n",
    "\n",
    "    return {w: list(contexts[w]) for w in simple_words if len(contexts[w]) >= max_context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49af1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sample_cosmopedia_texts(token_limit=TOKEN_LIMIT, subsets=COSMOPEDIA_SUBSETS)\n",
    "simple_words = find_simple_words(texts, wordbank_path=WORDBANK_PATH)\n",
    "print(f\"Found {len(simple_words)} simple words.\")\n",
    "contexts = collect_contexts_from_texts(texts, simple_words, max_context=X)\n",
    "print(f\"{len(contexts)} words have at least {X} contexts.\")\n",
    "with open(CONTEXTS_PKL_PATH, \"wb\") as f:\n",
    "    pickle.dump(contexts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500ba8a",
   "metadata": {},
   "source": [
    "## Load GPT-2 checkpoints and save surprisal + mean layer attention\n",
    "From the word contexts, we save the surprisal and mean layer attention for each word and training checkpoint. The GPT-2 checkpoints are loaded from HuggingFace/stanford-crfm which includes ~600 checkpoints up to training step 400,000.\n",
    "\n",
    "In the `Experiments` class, `compute_batches` takes all the contexts for a given word and turns them into prompts of form `\"<prefix> <word>\"`.\n",
    "\n",
    "For each prompt, the code finds the token positions corresponding to the target word (which may be multiple tokens). At each of those positions, it looks at the model’s predicted distribution before the token (i.e., logits at position −1), reads off the log-probability of the actual token, and sums these over all word tokens. Surprisal in bits is then defined as `-log p(word|prefix)`. A higher surprisal means the model found the word less expected in that context.\n",
    "\n",
    "From the attention tensors, we average how much attention all heads and all query positions of each layer pay into the positions of the target word tokens.\n",
    "\n",
    "We ran this section of the code on RunPod A100. Decrease number of contexts (`X`) or `TOKEN_LIMIT` above for a faster runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc40f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.loading_from_pretrained import get_checkpoint_labels\n",
    "from huggingface_hub.utils import RevisionNotFoundError\n",
    "\n",
    "MODEL_NAME = \"stanford-gpt2-small-a\"  # or \"stanford-gpt2-medium-a\"\n",
    "DTYPE = torch.float16\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 128\n",
    "CHECKPOINT_STRIDE = 12  # Loads a total of ~50 checkpoints\n",
    "MODEL_OUT_PATH = \"stanford-gpt2-small-a_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self,\n",
    "                 model: HookedTransformer,\n",
    "                 batch_size: int):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.device = str(self.model.cfg.device)\n",
    "        self.model.eval()\n",
    "        self.model.tokenizer.padding_side = \"right\"\n",
    "\n",
    "    def compute_batches(self,\n",
    "                        prefixes: List[str],\n",
    "                        word: str):\n",
    "        full_text_l = [prefix.rstrip() + \" \" + word for prefix in prefixes]\n",
    "        for i in range(0, len(full_text_l), self.batch_size):\n",
    "            yield {\n",
    "                \"prefixes\": prefixes[i:i + self.batch_size],\n",
    "                \"full_texts\": full_text_l[i:i + self.batch_size],\n",
    "            }\n",
    "\n",
    "    def compute_output(self,\n",
    "                       batch: Dict[str, List[str]]):\n",
    "        prefixes, full_texts = batch[\"prefixes\"], batch[\"full_texts\"]\n",
    "        batch_size = len(prefixes)\n",
    "\n",
    "        prefix_lens = [len(self.model.to_tokens(prefix, prepend_bos=False)[0]) for prefix in prefixes]\n",
    "        full_lens = [len(self.model.to_tokens(full_text, prepend_bos=False)[0]) for full_text in full_texts]\n",
    "        num_word_tokens = [full_len - pref_len for pref_len, full_len in zip(prefix_lens, full_lens)]\n",
    "\n",
    "        full_tokens = self.model.to_tokens(full_texts, prepend_bos=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, cache = self.model.run_with_cache(full_tokens)\n",
    "\n",
    "        total_logprob_e = torch.zeros(batch_size, device=self.device)\n",
    "        for i in range(batch_size):\n",
    "            if num_word_tokens[i] <= 0:\n",
    "                continue\n",
    "            word_token_positions = range(prefix_lens[i], full_lens[i])\n",
    "            for position in word_token_positions:\n",
    "                if position == 0:\n",
    "                    continue\n",
    "                logits_pos = logits[i, position - 1]\n",
    "                log_probs = F.log_softmax(logits_pos, dim=-1)\n",
    "                token_id = int(full_tokens[i, position].item())\n",
    "                total_logprob_e[i] += log_probs[token_id]\n",
    "\n",
    "        surprisal_bits = (-total_logprob_e / math.log(2.0)).tolist()\n",
    "\n",
    "        n_layers = self.model.cfg.n_layers\n",
    "        layer_avg_attn = torch.zeros(batch_size, n_layers, device=self.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if num_word_tokens[i] <= 0:\n",
    "                continue\n",
    "            word_token_positions = range(prefix_lens[i], full_lens[i])\n",
    "            for layer in range(n_layers):\n",
    "                attn = cache[\"attn\", layer][i]\n",
    "                into_word = attn[:, :, word_token_positions]\n",
    "                layer_avg_attn[i, layer] = into_word.mean()\n",
    "\n",
    "        del cache\n",
    "        if self.device.startswith(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return surprisal_bits, num_word_tokens, layer_avg_attn.cpu()\n",
    "\n",
    "    def compute_output_dict(self,\n",
    "                            contexts: Dict[str, List[str]]):\n",
    "        output_dict = {}\n",
    "        words = list(contexts.keys())\n",
    "\n",
    "        for word in tqdm(words):\n",
    "            prefixes = contexts[word]\n",
    "            surprisals = []\n",
    "            token_counts = []\n",
    "            layer_attn_vals = []\n",
    "\n",
    "            for batch in self.compute_batches(prefixes, word):\n",
    "                surprisal_bits, num_word_tokens, layer_avg_attn = self.compute_output(batch)\n",
    "                surprisals.extend(surprisal_bits)\n",
    "                token_counts.extend(num_word_tokens)\n",
    "                layer_attn_vals.append(layer_avg_attn)\n",
    "\n",
    "            if len(surprisals) == 0:\n",
    "                continue\n",
    "\n",
    "            layer_attn_vals = torch.cat(layer_attn_vals, dim=0)\n",
    "\n",
    "            avg = sum(surprisals) / len(surprisals)\n",
    "            per_token_vals = [s / t if t > 0 else float(\"nan\") for s, t in zip(surprisals, token_counts)]\n",
    "            per_token_vals = [x for x in per_token_vals if not math.isnan(x)]\n",
    "            if len(per_token_vals) > 0:\n",
    "                avg_per_token = sum(per_token_vals) / len(per_token_vals)\n",
    "            else:\n",
    "                avg_per_token = float(\"nan\")\n",
    "\n",
    "            avg_layer_attn = layer_attn_vals.mean(dim=0).tolist()\n",
    "\n",
    "            output_dict[word] = {\n",
    "                \"avg_surprisal\": avg,\n",
    "                \"avg_surprisal_per_token\": avg_per_token,\n",
    "                \"surprisals_list\": surprisals,\n",
    "                \"avg_layer_attn\": avg_layer_attn,\n",
    "            }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_OUT_PATH, exist_ok=True)\n",
    "labels, label_type = get_checkpoint_labels(MODEL_NAME)\n",
    "num_ckpts = len(labels)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Checkpoint label type: {label_type}\")\n",
    "print(f\"Total checkpoints in table: {num_ckpts}\")\n",
    "print(f\"Using stride {CHECKPOINT_STRIDE}\")\n",
    "\n",
    "with open(CONTEXTS_PKL_PATH, \"rb\") as f:\n",
    "    contexts = pickle.load(f)\n",
    "\n",
    "for idx in range(0, num_ckpts, CHECKPOINT_STRIDE):\n",
    "    label = labels[idx]\n",
    "    out_pkl = os.path.join(\n",
    "        MODEL_OUT_PATH,\n",
    "        f\"results_ckpt_idx{idx:04d}_{label_type}{label}.pkl\",\n",
    "    )\n",
    "\n",
    "    if os.path.exists(out_pkl):\n",
    "        print(f\"[{idx}] Skipping (results already exist): {out_pkl}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{idx}] Loading model {MODEL_NAME} checkpoint_index={idx} (label={label_type} {label})\")\n",
    "\n",
    "    try:\n",
    "        model = HookedTransformer.from_pretrained_no_processing(\n",
    "            MODEL_NAME,\n",
    "            checkpoint_index=idx,\n",
    "            device=DEVICE,\n",
    "            dtype=DTYPE,\n",
    "        )\n",
    "    except RevisionNotFoundError as e:\n",
    "        print(f\"[{idx}] SKIP: Revision not found on Hugging Face: {e}\")\n",
    "        continue\n",
    "    except OSError as e:\n",
    "        msg = str(e)\n",
    "        if \"not a valid git identifier\" in msg:\n",
    "            print(f\"[{idx}] SKIP: Invalid git revision for this checkpoint: {msg}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"[{idx}] ERROR: OSError when loading checkpoint: {msg}\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}] ERROR: Unexpected exception: {e}\")\n",
    "        continue\n",
    "\n",
    "    experiment = Experiment(model=model, batch_size=BATCH_SIZE)\n",
    "    output_dict = experiment.compute_output_dict(contexts)\n",
    "\n",
    "    with open(out_pkl, \"wb\") as f_out:\n",
    "        pickle.dump(output_dict, f_out)\n",
    "\n",
    "    del model\n",
    "    if DEVICE.startswith(\"cuda\"):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"[{idx}] Saved results to: {out_pkl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6f52e",
   "metadata": {},
   "source": [
    "## Plots\n",
    "We generate plots of the average attention and average surprisal for GPT-2 small and medium across training checkpoints. This includes a plot for the top 10, 100, and 500 simple words.\n",
    "\n",
    "We also generate a merged plot of the change in surprisal for each word (`margin_idx=2` indexes after crossing the AoA threshold), the proportion of children who have learned to a produce the word, and an overlay of both plots aligned by the AoA threshold (child AoA threshold is 0.5).\n",
    "\n",
    "Finally, we generate the same merged plot for change in mean layer attention. These plots are not cropped or aligned in any particular way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "SMALL_DIR = \"stanford-gpt2-small-a_results\"\n",
    "MEDIUM_DIR = \"stanford-gpt2-medium-a_results\"\n",
    "FIG_OUT_DIR = \"figs\"\n",
    "KS = [10, 100, 500]\n",
    "BASELINE_BITS = 15.6\n",
    "NUM_PLOTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36435b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cdi_label(w: str) -> str:\n",
    "    w = w.lower().strip()\n",
    "    w = re.sub(r\"\\s*\\(.*?\\)\\s*\", \"\", w)\n",
    "    w = re.sub(r\"\\s+\", \" \", w)\n",
    "    return w\n",
    "\n",
    "\n",
    "def load_wordbank_aoa(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    aoa_cols = [c for c in df.columns if c.isdigit()]\n",
    "    aoa_cols_sorted = sorted(aoa_cols, key=lambda x: int(x))\n",
    "    word_to_aoa = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = normalize_cdi_label(str(row[\"item_definition\"]))\n",
    "        if not word:\n",
    "            continue\n",
    "        aoa = math.nan\n",
    "        for col in aoa_cols_sorted:\n",
    "            try:\n",
    "                v = float(row[col])\n",
    "            except (TypeError, ValueError):\n",
    "                continue\n",
    "            if v >= 0.5:\n",
    "                aoa = float(col)\n",
    "                break\n",
    "        if not math.isnan(aoa):\n",
    "            if word in word_to_aoa:\n",
    "                word_to_aoa[word] = statistics.mean([word_to_aoa[word], aoa])\n",
    "            else:\n",
    "                word_to_aoa[word] = aoa\n",
    "    return word_to_aoa\n",
    "\n",
    "\n",
    "def load_wordbank_curves(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    aoa_cols = [c for c in df.columns if c.isdigit()]\n",
    "    aoa_cols_sorted = sorted(aoa_cols, key=lambda x: int(x))\n",
    "    months = [int(c) for c in aoa_cols_sorted]\n",
    "    word_to_curve: Dict[str, np.ndarray] = {}\n",
    "    for _, row in df.iterrows():\n",
    "        word = normalize_cdi_label(str(row[\"item_definition\"]))\n",
    "        if not word:\n",
    "            continue\n",
    "        vals = []\n",
    "        for col in aoa_cols_sorted:\n",
    "            v = row[col]\n",
    "            try:\n",
    "                vals.append(float(v))\n",
    "            except (TypeError, ValueError):\n",
    "                vals.append(math.nan)\n",
    "        vals_arr = np.array(vals, dtype=float)\n",
    "        if word in word_to_curve:\n",
    "            prev = word_to_curve[word]\n",
    "            stacked = np.vstack([prev, vals_arr])\n",
    "            word_to_curve[word] = np.nanmean(stacked, axis=0)\n",
    "        else:\n",
    "            word_to_curve[word] = vals_arr\n",
    "    return months, word_to_curve\n",
    "\n",
    "\n",
    "def parse_step_from_fname(fname: str):\n",
    "    m_label = re.search(r\"idx\\d+_([A-Za-z]+)(\\d+)\\.pkl$\", fname)\n",
    "    if not m_label:\n",
    "        raise ValueError(f\"Cannot parse label from {fname}\")\n",
    "    step = int(m_label.group(2))\n",
    "    return step\n",
    "\n",
    "\n",
    "def load_results_dir(results_dir: str):\n",
    "    entries = []\n",
    "    for fname in os.listdir(results_dir):\n",
    "        if not fname.endswith(\".pkl\"):\n",
    "            continue\n",
    "        step = parse_step_from_fname(fname)\n",
    "        path = os.path.join(results_dir, fname)\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        word_surprisal = {}\n",
    "        word_act = {}\n",
    "        for w, info in data.items():\n",
    "            w_norm = normalize_cdi_label(w)\n",
    "            word_surprisal[w_norm] = float(info[\"avg_surprisal\"])\n",
    "            layer_attn = info.get(\"avg_layer_attn\", None)\n",
    "            if layer_attn is not None:\n",
    "                layer_arr = np.array(layer_attn, dtype=float)\n",
    "                word_act[w_norm] = float(np.nanmean(layer_arr))\n",
    "        entries.append((step, word_surprisal, word_act))\n",
    "    entries.sort(key=lambda x: x[0])\n",
    "    steps = [e[0] for e in entries]\n",
    "    all_words_surprisal: Set[str] = set()\n",
    "    all_words_act: Set[str] = set()\n",
    "    for _, ws, wa in entries:\n",
    "        all_words_surprisal.update(ws.keys())\n",
    "        all_words_act.update(wa.keys())\n",
    "    all_words = all_words_surprisal | all_words_act\n",
    "    word_to_surprisal: Dict[str, List[float]] = {}\n",
    "    word_to_act: Dict[str, List[float]] = {}\n",
    "    for w in all_words:\n",
    "        s_series = []\n",
    "        a_series = []\n",
    "        for _, ws, wa in entries:\n",
    "            s_series.append(float(ws.get(w, math.nan)))\n",
    "            a_series.append(float(wa.get(w, math.nan)))\n",
    "        word_to_surprisal[w] = s_series\n",
    "        word_to_act[w] = a_series\n",
    "    return steps, word_to_surprisal, word_to_act\n",
    "\n",
    "\n",
    "def get_simple_ranking(word_aoa: Dict[str, float], available_words: set[str], max_n: int):\n",
    "    items = [(w, aoa) for w, aoa in word_aoa.items() if w in available_words and not math.isnan(aoa)]\n",
    "    items.sort(key=lambda x: (x[1], x[0]))\n",
    "    return [w for w, _ in items[:max_n]]\n",
    "\n",
    "\n",
    "def compute_avg_series(word_to_series: Dict[str, List[float]], words: List[str]):\n",
    "    if not words:\n",
    "        return np.array([], dtype=float)\n",
    "    arr = []\n",
    "    for w in words:\n",
    "        arr.append(np.array(word_to_series[w], dtype=float))\n",
    "    arr = np.stack(arr, axis=0)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        return np.nanmean(arr, axis=0)\n",
    "\n",
    "\n",
    "def compute_thresholds_per_word(\n",
    "    word_to_series: Dict[str, List[float]],\n",
    "    baseline_bits: float,\n",
    "    words: List[str],\n",
    "    steps: List[int],\n",
    "):\n",
    "    aoa_log10 = compute_llm_aoa_steps(\n",
    "        word_to_series=word_to_series,\n",
    "        steps=steps,\n",
    "        baseline_bits=baseline_bits,\n",
    "        words=words,\n",
    "    )\n",
    "    aoa_steps = {}\n",
    "    for w, x_star in aoa_log10.items():\n",
    "        aoa_steps[w] = float(10.0 ** x_star)\n",
    "    return aoa_steps\n",
    "\n",
    "\n",
    "def logistic4(x, L, k, x0, b):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    z = k * (x - x0)\n",
    "    z = np.clip(z, -60, 60)\n",
    "    return L / (1.0 + np.exp(z)) + b\n",
    "\n",
    "\n",
    "def compute_llm_aoa_steps(\n",
    "    word_to_series: Dict[str, List[float]],\n",
    "    steps: List[int],\n",
    "    baseline_bits: float,\n",
    "    words: List[str],\n",
    "):\n",
    "    aoa_log10: Dict[str, float] = {}\n",
    "    if not words:\n",
    "        return aoa_log10\n",
    "\n",
    "    step_arr = np.array(steps, dtype=float)\n",
    "    safe_steps = np.where(step_arr > 0, step_arr, 1.0)\n",
    "    log_steps = np.log10(safe_steps)\n",
    "    mask_log = np.isfinite(log_steps)\n",
    "\n",
    "    for w in words:\n",
    "        s = np.array(word_to_series[w], dtype=float)\n",
    "        mask = mask_log & np.isfinite(s)\n",
    "        x = log_steps[mask]\n",
    "        y = s[mask]\n",
    "        y_min = float(np.min(y))\n",
    "        y_max = float(np.max(y))\n",
    "        if not np.isfinite(y_min) or not np.isfinite(y_max) or y_max == y_min:\n",
    "            continue\n",
    "\n",
    "        thr = 0.5 * (baseline_bits + y_min)\n",
    "\n",
    "        L0 = max(y_max - y_min, 1e-3)\n",
    "        b0 = y_min\n",
    "        x0_0 = float(np.median(x))\n",
    "        k0 = 1.0\n",
    "\n",
    "        x_star = None\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(\n",
    "                logistic4,\n",
    "                x,\n",
    "                y,\n",
    "                p0=[L0, k0, x0_0, b0],\n",
    "                maxfev=10000,\n",
    "            )\n",
    "            L, k, x0, b = popt\n",
    "\n",
    "            if L * k == 0:\n",
    "                raise RuntimeError(\"flat logistic fit\")\n",
    "\n",
    "            f_lo = float(logistic4(x[0], *popt))\n",
    "            f_hi = float(logistic4(x[-1], *popt))\n",
    "\n",
    "            lo_val = min(f_lo, f_hi)\n",
    "            hi_val = max(f_lo, f_hi)\n",
    "\n",
    "            if not (lo_val <= thr <= hi_val):\n",
    "                raise RuntimeError(\"threshold outside fit range\")\n",
    "\n",
    "            lo_x = x[0]\n",
    "            hi_x = x[-1]\n",
    "\n",
    "            if f_lo <= f_hi:\n",
    "                for _ in range(60):\n",
    "                    mid_x = 0.5 * (lo_x + hi_x)\n",
    "                    val = float(logistic4(mid_x, *popt))\n",
    "                    if val < thr:\n",
    "                        lo_x = mid_x\n",
    "                    else:\n",
    "                        hi_x = mid_x\n",
    "            else:\n",
    "                for _ in range(60):\n",
    "                    mid_x = 0.5 * (lo_x + hi_x)\n",
    "                    val = float(logistic4(mid_x, *popt))\n",
    "                    if val > thr:\n",
    "                        lo_x = mid_x\n",
    "                    else:\n",
    "                        hi_x = mid_x\n",
    "\n",
    "            x_star = 0.5 * (lo_x + hi_x)\n",
    "\n",
    "        except Exception:\n",
    "            idx_val = None\n",
    "\n",
    "            for j in range(1, len(y)):\n",
    "                y_prev, y_curr = y[j - 1], y[j]\n",
    "                if not (np.isfinite(y_prev) and np.isfinite(y_curr)):\n",
    "                    continue\n",
    "\n",
    "                if (y_prev - thr) * (y_curr - thr) <= 0:\n",
    "                    idx_val = j\n",
    "                    break\n",
    "\n",
    "            if idx_val is None:\n",
    "                idx_val = int(np.argmin(np.abs(y - thr)))\n",
    "            x_star = float(x[idx_val])\n",
    "\n",
    "        if x_star is not None and np.isfinite(x_star):\n",
    "            aoa_log10[w] = x_star\n",
    "\n",
    "    return aoa_log10\n",
    "\n",
    "\n",
    "def normalize_x(xs: np.ndarray):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    if xs.size == 0:\n",
    "        return xs\n",
    "    denom = xs[-1] - xs[0]\n",
    "    if denom <= 0:\n",
    "        return np.zeros_like(xs)\n",
    "    return (xs - xs[0]) / denom\n",
    "\n",
    "\n",
    "def compute_child_interp_aoa(wordbank_csv: str):\n",
    "    months, word_to_curve = load_wordbank_curves(wordbank_csv)\n",
    "    months_arr = np.array(months, dtype=float)\n",
    "\n",
    "    child_interp_aoa = {}\n",
    "    for w, curve in word_to_curve.items():\n",
    "        curve_arr = np.array(curve, dtype=float)\n",
    "        if not np.isfinite(curve_arr).any():\n",
    "            continue\n",
    "\n",
    "        mask = np.isfinite(curve_arr)\n",
    "        m = months_arr[mask]\n",
    "        y = curve_arr[mask]\n",
    "        if m.size < 2:\n",
    "            continue\n",
    "\n",
    "        idx_cross = None\n",
    "        for j in range(1, y.size):\n",
    "            y_prev = y[j - 1]\n",
    "            y_curr = y[j]\n",
    "            if not (np.isfinite(y_prev) and np.isfinite(y_curr)):\n",
    "                continue\n",
    "            if (y_prev - 0.5) * (y_curr - 0.5) <= 0:\n",
    "                idx_cross = j\n",
    "                break\n",
    "\n",
    "        if idx_cross is None:\n",
    "            continue\n",
    "\n",
    "        m1, m2 = m[idx_cross - 1], m[idx_cross]\n",
    "        y1, y2 = y[idx_cross - 1], y[idx_cross]\n",
    "        if not (np.isfinite(y1) and np.isfinite(y2)) or m2 == m1:\n",
    "            continue\n",
    "\n",
    "        if y2 == y1:\n",
    "            aoa_month = m2\n",
    "        else:\n",
    "            aoa_month = m1 + (0.5 - y1) * (m2 - m1) / (y2 - y1)\n",
    "\n",
    "        if np.isfinite(aoa_month) and aoa_month > 0:\n",
    "            child_interp_aoa[w] = float(aoa_month)\n",
    "\n",
    "    return child_interp_aoa\n",
    "\n",
    "\n",
    "def normalize_x_aligned(xs, align_idx, child_aoa_x):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    if xs.size == 0 or align_idx >= len(xs) or align_idx < 0:\n",
    "        return normalize_x(xs)\n",
    "    \n",
    "    x_align = xs[align_idx]\n",
    "    x_start = xs[0]\n",
    "    x_end = xs[-1]\n",
    "    xs_norm = (xs - x_start) / (x_end - x_start)\n",
    "    align_norm = (x_align - x_start) / (x_end - x_start)\n",
    "\n",
    "    if align_norm == 0 or not np.isfinite(child_aoa_x) or child_aoa_x == 0:\n",
    "        return xs_norm\n",
    "\n",
    "    xs_aligned = xs_norm * (child_aoa_x / align_norm)\n",
    "    \n",
    "    return xs_aligned\n",
    "\n",
    "\n",
    "def compute_threshold_crossing_idx(s, baseline_bits):\n",
    "    s = np.array(s, dtype=float)\n",
    "    if not np.isfinite(s).any():\n",
    "        return None\n",
    "    s_min = float(np.nanmin(s))\n",
    "    thr = 0.5 * (baseline_bits + s_min)\n",
    "    \n",
    "    for j, v in enumerate(s):\n",
    "        if np.isfinite(v) and v <= thr:\n",
    "            return j\n",
    "    return len(s) - 1\n",
    "\n",
    "\n",
    "def crop_with_threshold(s, steps_arr, baseline_bits, margin_idx):\n",
    "    s = np.array(s, dtype=float)\n",
    "    if not np.isfinite(s).any():\n",
    "        return None, None, None, None\n",
    "    s_min = float(np.nanmin(s))\n",
    "    thr = 0.5 * (baseline_bits + s_min)\n",
    "    idx_cross = None\n",
    "    for j, v in enumerate(s):\n",
    "        if np.isfinite(v) and v <= thr:\n",
    "            idx_cross = j\n",
    "            break\n",
    "    if idx_cross is None:\n",
    "        idx_cross = len(s) - 1\n",
    "    end_idx = min(idx_cross + margin_idx, len(s) - 1)\n",
    "    s_crop = s[: end_idx + 1]\n",
    "    steps_crop = steps_arr[: end_idx + 1]\n",
    "    return s_crop, steps_crop, thr, idx_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(FIG_OUT_DIR, exist_ok=True)\n",
    "\n",
    "word_aoa = load_wordbank_aoa(WORDBANK_PATH)\n",
    "months, word_to_curve = load_wordbank_curves(WORDBANK_PATH)\n",
    "\n",
    "steps_small, small_surpr = load_results_dir(SMALL_DIR)\n",
    "steps_medium, medium_surpr = load_results_dir(MEDIUM_DIR)\n",
    "\n",
    "words_small = set(small_surpr.keys())\n",
    "words_medium = set(medium_surpr.keys())\n",
    "available_words = words_small & words_medium\n",
    "\n",
    "simple_ranking = get_simple_ranking(word_aoa, available_words, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617257b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Child trajectories vs LLM surprisal (per word)\n",
    "if simple_ranking:\n",
    "    max_words_side_by_side = 10\n",
    "    n_plotted = 0\n",
    "    margin_idx = 3\n",
    "    months_arr = np.array(months, dtype=float)\n",
    "    if months_arr.size > 1:\n",
    "        months_norm = (months_arr - months_arr.min()) / (months_arr.max() - months_arr.min())\n",
    "    else:\n",
    "        months_norm = np.zeros_like(months_arr)\n",
    "\n",
    "    for w in simple_ranking[150:]:\n",
    "        if w not in word_to_curve:\n",
    "            continue\n",
    "        child_curve = word_to_curve[w]\n",
    "        if not np.isfinite(child_curve).any():\n",
    "            continue\n",
    "        s_small = np.array(small_surpr[w], dtype=float)\n",
    "        s_medium = np.array(medium_surpr[w], dtype=float)\n",
    "        if not (np.isfinite(s_small).any() or np.isfinite(s_medium).any()):\n",
    "            continue\n",
    "\n",
    "        steps_small_arr = np.array(steps_small, dtype=float)\n",
    "        steps_medium_arr = np.array(steps_medium, dtype=float)\n",
    "\n",
    "        s_small_crop, steps_small_crop, thr_small, idx_cross_small = crop_with_threshold(\n",
    "            s_small, steps_small_arr, args.baseline_bits, margin_idx\n",
    "        )\n",
    "        s_medium_crop, steps_medium_crop, thr_medium, idx_cross_medium = crop_with_threshold(\n",
    "            s_medium, steps_medium_arr, args.baseline_bits, margin_idx\n",
    "        )\n",
    "\n",
    "        if s_small_crop is None and s_medium_crop is None:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "        if s_small_crop is not None and len(s_small_crop) > 0:\n",
    "            axes[0].plot(steps_small_crop, s_small_crop, label=\"gpt2-small\")\n",
    "            if thr_small is not None:\n",
    "                axes[0].axhline(thr_small, linestyle=\"--\", linewidth=1, color=\"cornflowerblue\")\n",
    "        if s_medium_crop is not None and len(s_medium_crop) > 0:\n",
    "            axes[0].plot(steps_medium_crop, s_medium_crop, label=\"gpt2-medium\")\n",
    "            if thr_medium is not None:\n",
    "                axes[0].axhline(thr_medium, linestyle=\"--\", linewidth=1, color=\"orange\")\n",
    "\n",
    "        axes[0].set_xlabel(\"Step\")\n",
    "        axes[0].set_ylabel(\"Surprisal (bits)\")\n",
    "        axes[0].set_title(f\"{w} - LLM surprisal\")\n",
    "        axes[0].legend()\n",
    "        axes[0].invert_yaxis()\n",
    "\n",
    "        axes[1].plot(months, child_curve, marker=\"o\")\n",
    "        axes[1].axhline(0.5, linestyle=\"--\", linewidth=1, color=\"cornflowerblue\")\n",
    "        axes[1].set_xlabel(\"Age (months)\")\n",
    "        axes[1].set_ylabel(\"Proportion producing\")\n",
    "        axes[1].set_ylim(0.0, 1.0)\n",
    "        axes[1].set_title(f\"{w} - Children\")\n",
    "\n",
    "        ax3 = axes[2]\n",
    "        ax3b = ax3.twinx()\n",
    "\n",
    "        # Align AoA\n",
    "        child_aoa_x = None\n",
    "        if w in compute_child_interp_aoa:\n",
    "            child_aoa_month = compute_child_interp_aoa[w]\n",
    "            if months_arr.size > 1:\n",
    "                child_aoa_x = (child_aoa_month - months_arr.min()) / (months_arr.max() - months_arr.min())\n",
    "                child_aoa_x = np.clip(child_aoa_x, 0.0, 1.0)\n",
    "        else:\n",
    "            child_mask = np.isfinite(child_curve)\n",
    "            if child_mask.any():\n",
    "                first_valid_val = child_curve[child_mask][0]\n",
    "                if first_valid_val >= 0.5:\n",
    "                    child_aoa_x = 0.0\n",
    "                else:\n",
    "                    plt.close(fig)\n",
    "                    continue\n",
    "\n",
    "        if child_aoa_x is None:\n",
    "            plt.close(fig)\n",
    "            continue\n",
    "\n",
    "        if np.isfinite(s_small).any() and idx_cross_small is not None:\n",
    "            x_small_norm = normalize_x_aligned(steps_small_arr, idx_cross_small, child_aoa_x)\n",
    "            ax3.plot(x_small_norm, s_small, label=\"gpt2-small\")\n",
    "        if np.isfinite(s_medium).any() and idx_cross_medium is not None:\n",
    "            x_medium_norm = normalize_x_aligned(steps_medium_arr, idx_cross_medium, child_aoa_x)\n",
    "            ax3.plot(x_medium_norm, s_medium, label=\"gpt2-medium\")\n",
    "\n",
    "        child_mask = np.isfinite(child_curve)\n",
    "        if child_mask.any():\n",
    "            ax3b.plot(months_norm[child_mask], child_curve[child_mask], marker=\"o\", color=\"green\", label=\"children\")\n",
    "\n",
    "        all_s_vals = []\n",
    "        for arr in (s_small, s_medium):\n",
    "            if arr is not None:\n",
    "                arr = np.asarray(arr, dtype=float)\n",
    "                arr = arr[np.isfinite(arr)]\n",
    "                if arr.size > 0:\n",
    "                    all_s_vals.extend(arr.tolist())\n",
    "\n",
    "        # Align AoA threshold\n",
    "        if all_s_vals:\n",
    "            min_surprisal = float(min(all_s_vals))\n",
    "            max_surprisal = float(max(all_s_vals))\n",
    "            data_range = max_surprisal - min_surprisal\n",
    "            if data_range <= 0:\n",
    "                data_range = 1.0\n",
    "            buffer = 0.1 * data_range\n",
    "            half_total = 0.5 * data_range + buffer\n",
    "            y_max = thr_small + half_total\n",
    "            y_min = thr_small - half_total\n",
    "            ax3.set_ylim(y_max, y_min)\n",
    "\n",
    "        ax3.set_xlim(0.0, 1.0)\n",
    "        ax3.set_xlabel(\"Normalized timeline\")\n",
    "        ax3.set_ylabel(\"Surprisal (bits)\")\n",
    "        ax3b.set_ylabel(\"Proportion producing\")\n",
    "        ax3b.set_ylim(0.0, 1.0)\n",
    "        ax3.set_title(f\"{w} - Normalized aligned overlay\")\n",
    "\n",
    "        handles1, labels1 = ax3.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax3b.get_legend_handles_labels()\n",
    "        if handles1 or handles2:\n",
    "            ax3.legend(handles1 + handles2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        safe_w = re.sub(r\"[^A-Za-z0-9]+\", \"_\", w).strip(\"_\")\n",
    "        out_path_word = os.path.join(args.out_dir, f\"word_{safe_w}_child_vs_llm_surprisal.png\")\n",
    "        fig.savefig(out_path_word, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        n_plotted += 1\n",
    "        if n_plotted >= max_words_side_by_side:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
